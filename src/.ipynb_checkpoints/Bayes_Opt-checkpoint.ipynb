{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292404cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import metrics\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9bd8545-120c-470b-a0a6-8dbdbca5ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters:\n",
    "# -FF\n",
    "# num layers\n",
    "# size of layers\n",
    "# Batch Norm\n",
    "\n",
    "# -LSTM\n",
    "# num layers\n",
    "# size of layers\n",
    "# Batch Norm\n",
    "\n",
    "# -v\n",
    "# \n",
    "# -d\n",
    "# softplus factor rho\n",
    "# \n",
    "# -m\n",
    "# posterior softplus factor rho\n",
    "# prior std\n",
    "# size of dense variational layer\n",
    "# DistributionLambda scale\n",
    "\n",
    "# -c\n",
    "# posterior softplus factor rho\n",
    "# prior std\n",
    "# size of dense variational layer\n",
    "# softplus factor rho\n",
    "\n",
    "# args=  Namespace(Arch='FF', \n",
    "#                  num_layers=1,\n",
    "#                  sizeof_layers=256,\n",
    "#                  batch_norm=True,\n",
    "#                  Ext='-c', \n",
    "#                  kl_anneal=0.1,\n",
    "#                  rho_q=10,\n",
    "#                  rho_op=0.25,\n",
    "#                  prior_scale=0.1,\n",
    "#                  sizeof_bnn=25,\n",
    "#                  Batch_Size=64, \n",
    "#                  Epochs=5, \n",
    "#                  Gamma=14, \n",
    "#                  country='us', \n",
    "#                  early_stopping=False, \n",
    "#                  smooth=True,\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e396cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def list_dates(start = '2017-08-23', end = '2017-08-23', window_size=28):\n",
    "    delta = (dt.datetime.strptime(end, '%Y-%m-%d') - dt.datetime.strptime(start, '%Y-%m-%d')).days\n",
    "    dates=[]\n",
    "    for i in range(delta+window_size):\n",
    "        dates.append(dt.datetime.strptime(start, '%Y-%m-%d') - dt.timedelta(days=window_size) + dt.timedelta(days=i))\n",
    "    return np.asarray(dates)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dba9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c54578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(df, data):\n",
    "    unscaled = pd.DataFrame(index=df.index)\n",
    "\n",
    "    true = df['True'].values\n",
    "    y_hat = df['Pred'].values\n",
    "\n",
    "\n",
    "    if 'Std' in df.columns:\n",
    "        std = df['Std'].values\n",
    "        std = data.scaler.inverse_transform(np.tile(std+true, (255,1)).T)[:,-1]\n",
    "\n",
    "\n",
    "    true =  data.scaler.inverse_transform(np.tile(true, (255,1)).T)[:,-1]\n",
    "    y_hat =  data.scaler.inverse_transform(np.tile(y_hat, (255,1)).T)[:,-1]\n",
    "\n",
    "\n",
    "    unscaled['True'] = true\n",
    "    unscaled['Pred'] = y_hat\n",
    "\n",
    "    if 'Std' in df.columns:\n",
    "        std = std-true\n",
    "        unscaled['Std'] = std\n",
    "    \n",
    "    return unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4db9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_smoothing(df, n=7):\n",
    "    data = df.values\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]-1, n, -1):\n",
    "            s = 1\n",
    "            for k in range(1,n):\n",
    "                data[i, j] = data[i,j] + data[i,j-n]/(k+1)\n",
    "                s = s + 1/(k+1)\n",
    "            data[i, j] = data[i,j]/s\n",
    "            \n",
    "    df = pd.DataFrame(columns = df.columns, index=df.index[-data.shape[0]:], data=data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1cc60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "class data_builder_new:\n",
    "    def __init__(self, root,country='eng'):\n",
    "        self.directory = root\n",
    "\n",
    "    def window(self, data, window_size=28):\n",
    "        windowed = []\n",
    "        for i in range(1+data.shape[0] - window_size):\n",
    "            windowed.append(data[i:i + window_size])\n",
    "        windowed = np.asarray(windowed)\n",
    "        return windowed\n",
    "\n",
    "    def build(self, test_year=2017, validation_year=2016, start_of_season = '-08-23', gamma=14, lag=14, window_size=28, smooth=True):\n",
    "            \n",
    "            if smooth:\n",
    "                Qs = pd.read_csv(self.directory + 'google_queries_pre_processed_us_smooth.csv', \n",
    "                                 index_col=0, \n",
    "                                 parse_dates=True)\n",
    "            else:\n",
    "                Qs = pd.read_csv(self.directory + 'google_queries_pre_processed_us.csv', \n",
    "                                 index_col=0, \n",
    "                                 parse_dates=True)\n",
    "            \n",
    "            ILI = pd.read_csv('/home/mimorris/Datasets/Flu/ILI_rates_US_wednesday_linear_interpolation.csv', \n",
    "                              parse_dates=True)\n",
    "            ILI = ILI.set_index('date')\n",
    "            ILI.index = pd.to_datetime(ILI.index)\n",
    "            \n",
    "            Qs = Qs[:ILI.index[-1]]\n",
    "            ILI.index = ILI.index + dt.timedelta(days=lag)\n",
    "            Qs['ILI'] = ILI.loc[Qs.index[0]:Qs.index[-1]]\n",
    "            \n",
    "            self.scaler = MinMaxScaler()\n",
    "            self.scaler.fit(Qs)\n",
    "            \n",
    "            window_size=window_size-1\n",
    "            \n",
    "            Qs = pd.DataFrame(index=Qs.index, columns=Qs.columns, data=self.scaler.transform(Qs))\n",
    "            \n",
    "            validation_index = list_dates(start=str(validation_year)+start_of_season, \n",
    "                                          end=str(validation_year+1)+start_of_season, \n",
    "                                          window_size=window_size)\n",
    "            \n",
    "            test_index = list_dates(start=str(test_year)+start_of_season, \n",
    "                                    end=str(test_year+1)+start_of_season,\n",
    "                                    window_size=window_size)\n",
    "            \n",
    "            train_index = list_dates(start=dt.datetime.strftime(Qs.index[gamma+window_size], '%Y-%m-%d'), \n",
    "                                     end=str(validation_year)+start_of_season,\n",
    "                                     window_size=window_size)\n",
    "            \n",
    "            x_train = Qs.loc[train_index - dt.timedelta(days=gamma)]\n",
    "            x_val = Qs.loc[validation_index - dt.timedelta(days=gamma)]\n",
    "            x_test = Qs.loc[test_index - dt.timedelta(days=gamma)]\n",
    "            \n",
    "            y_train = Qs.loc[(train_index + dt.timedelta(days=lag))[window_size:]]['ILI']\n",
    "            y_val = Qs.loc[(validation_index + dt.timedelta(days=lag))[window_size:]]['ILI']\n",
    "            y_test = Qs.loc[(test_index + dt.timedelta(days=lag))[window_size:]]['ILI']\n",
    "            \n",
    "            x_train = self.window(x_train, window_size=window_size+1)\n",
    "            x_val = self.window(x_val, window_size=window_size+1)\n",
    "            x_test = self.window(x_test, window_size=window_size+1)\n",
    "            \n",
    "            \n",
    "            return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3353318-8df0-4179-9c4a-5ba7d14946e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class results_table:\n",
    "    def __init__(self, args):\n",
    "        self.test_predictions = pd.DataFrame()\n",
    "        self.test_metrics = pd.DataFrame(index = ['CRPS','NLL','MAE','RMSE','SMAPE','Corr','MB Log','SDP'])\n",
    "        \n",
    "        self.val_predictions = pd.DataFrame()\n",
    "        self.val_metrics = pd.DataFrame(index = ['CRPS','NLL','MAE','RMSE','SMAPE','Corr','MB Log','SDP'])\n",
    "        \n",
    "        self.args = ['Arch='+str(args.Arch), \n",
    "                        'num_layers='+str(args.num_layers), \n",
    "                        'sizeof_layers='+str(args.sizeof_layers),\n",
    "                        'batch_norm='+str(args.batch_norm),\n",
    "                        'Ext='+str(args.Ext), \n",
    "                        'kl_anneal='+str(args.kl_anneal),\n",
    "                        'rho_q='+str(args.rho_q),\n",
    "                        'rho_op='+str(args.rho_op),\n",
    "                        'prior_scale='+str(args.prior_scale),\n",
    "                        'sizeof_bnn='+str(args.sizeof_bnn),\n",
    "                        'Batch_Size='+str(args.Batch_Size), \n",
    "                        'Epochs='+str(args.Epochs), \n",
    "                        'Gamma='+str(args.Gamma), \n",
    "                        'country='+str(args.country), \n",
    "                        'early_stopping='+str(args.early_stopping), \n",
    "                        'smooth='+str(args.smooth)]\n",
    "        \n",
    "    def update(self, test_predictions, val_predictions, year):\n",
    "        self.test_predictions = self.test_predictions.append(test_predictions)\n",
    "        \n",
    "        try:\n",
    "            self.test_metrics[str(year + 2014) + '/' + str(year + 15)] = [metrics.crps(test_predictions),\n",
    "                                                        metrics.nll(test_predictions),\n",
    "                                                        metrics.mae(test_predictions),\n",
    "                                                        metrics.rmse(test_predictions),\n",
    "                                                        metrics.smape(test_predictions),\n",
    "                                                        metrics.corr(test_predictions),\n",
    "                                                        metrics.mb_log(test_predictions).mean(),\n",
    "                                                        metrics.sdp(test_predictions)]\n",
    "        except:\n",
    "            pass\n",
    "        if year == 3:\n",
    "            self.test_metrics['Average'] = self.test_metrics.mean(1)\n",
    "            self.test_metrics['Average'].loc['SDP'] = np.abs(self.test_metrics.loc['SDP'].values[-1]).mean()\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            self.val_predictions= self.val_predictions.append(val_predictions)\n",
    "            self.val_metrics[str(year + 2013) + '/' + str(year + 14)] = [metrics.crps(val_predictions),\n",
    "                                                            metrics.nll(val_predictions),\n",
    "                                                            metrics.mae(val_predictions),\n",
    "                                                            metrics.rmse(val_predictions),\n",
    "                                                            metrics.smape(val_predictions),\n",
    "                                                            metrics.corr(val_predictions),\n",
    "                                                            metrics.mb_log(val_predictions).mean(),\n",
    "                                                            metrics.sdp(val_predictions)]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.val_metrics['Average'] = self.val_metrics.mean(1)\n",
    "        self.val_metrics['Average'].loc['SDP'] = np.abs(self.val_metrics.loc['SDP'].values[-1]).mean()\n",
    "        self.test_metrics['Average'] = self.test_metrics.mean(1)\n",
    "        self.test_metrics['Average'].loc['SDP'] = np.abs(self.test_metrics.loc['SDP'].values[-1]).mean()\n",
    "    \n",
    "    def save(self, dir, extension):\n",
    "        root = os.getcwd()\n",
    "        os.chdir(dir)\n",
    "        \n",
    "        os.mkdir(str(extension))\n",
    "        os.chdir(str(extension))\n",
    "        self.val_metrics.to_csv('val_metrics.csv')\n",
    "        self.val_predictions.to_csv('val_predictions.csv')\n",
    "        self.test_metrics.to_csv('test_metrics.csv')\n",
    "        self.test_predictions.to_csv('test_predictions.csv')\n",
    "        \n",
    "        with open('args.txt', 'w') as f:\n",
    "            for item in self.args:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        \n",
    "        os.chdir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad70591b-47a2-40ad-bf53-47cdbdc5d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num():\n",
    "    nums = []\n",
    "    for i in np.asarray(os.listdir('results/hyper_parameter_optimisation/')):\n",
    "        try:\n",
    "            nums.append(int(i))\n",
    "        except:\n",
    "            pass\n",
    "    return(np.asarray(nums).max()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64be663c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import model\n",
    "importlib.reload(model)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def run_model(kl_anneal, num_layers, prior_scale, rho_op, rho_q, sizeof_bnn, sizeof_layers):\n",
    "    os.chdir('/home/mimorris/Projects/Neural-Network-Flu-Uncertainty/src')\n",
    "    args = Namespace(Arch='FF', \n",
    "                 num_layers=int(num_layers),\n",
    "                 sizeof_layers=int(sizeof_layers),\n",
    "                 batch_norm=True,\n",
    "                 Ext='-c', \n",
    "                 kl_anneal=float(kl_anneal),\n",
    "                 rho_q=float(rho_q),\n",
    "                 rho_op=float(rho_op),\n",
    "                 prior_scale=float(prior_scale),\n",
    "                 sizeof_bnn=int(sizeof_bnn),\n",
    "                 Batch_Size=64, \n",
    "                 Epochs=100, \n",
    "                 Gamma=0, \n",
    "                 country='us', \n",
    "                 early_stopping=False, \n",
    "                 smooth=True,\n",
    "                )\n",
    "    \n",
    "    data = data_builder_new(root=\"/home/mimorris/Datasets/Flu/\", \n",
    "                            country = args.country)\n",
    "    \n",
    "    results = results_table(args)\n",
    "\n",
    "    for year in range(1):\n",
    "        try:\n",
    "            tf.keras.backend.clear_session()\n",
    "            x_train, y_train, x_val, y_val, x_test, y_test = data.build(test_year=2014+year, \n",
    "                                                                    validation_year=2013+year, \n",
    "                                                                    start_of_season = '-08-23', \n",
    "                                                                    gamma=args.Gamma, \n",
    "                                                                    lag=14, \n",
    "                                                                    window_size=28,\n",
    "                                                                    smooth=args.smooth)\n",
    "\n",
    "            NN = model.model_builder(x_train, y_train.values[:, np.newaxis], args=args)\n",
    "            NN.fit(x_train, y_train.values[:, np.newaxis], verbose=False)\n",
    "\n",
    "            val_pred = NN.predict(x_val, pd.DataFrame(index=y_val.index, data=y_val.values, columns=['T0']))\n",
    "            test_pred = NN.predict(x_test, pd.DataFrame(index=y_test.index, data=y_test.values, columns=['T0']))\n",
    "\n",
    "            val_pred = rescale(val_pred, data)\n",
    "            test_pred = rescale(test_pred, data)\n",
    "            results.update(test_pred, val_pred, year)\n",
    "        except:\n",
    "            return -100\n",
    "        \n",
    "    results.save('results/hyper_parameter_optimisation/', get_num())\n",
    "    os.listdir('results/hyper_parameter_optimisation/')\n",
    "    \n",
    "    if not np.isnan( -results.val_metrics['Average']['MB Log']):\n",
    "        return(results.val_metrics['Average']['MB Log'])\n",
    "    else:\n",
    "        return -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae716d61-45ac-4c31-9103-c1d06ffa529a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "pbounds = {'kl_anneal': (0.1, 5.0),\n",
    "           'num_layers': (1, 4), \n",
    "           'prior_scale':(0.01, 10), \n",
    "           'rho_op':(0.1, 10),\n",
    "           'rho_q':(0.1, 10),\n",
    "           'sizeof_bnn':(2, 128),\n",
    "           'sizeof_layers': (16, 512)\n",
    "          }\n",
    "           \n",
    "optimizer = BayesianOptimization(\n",
    "    f=run_model,\n",
    "    pbounds=pbounds,\n",
    "    random_state=10,\n",
    "    verbose=2\n",
    ")           \n",
    "# load_logs(optimizer, logs=[\"./logs.json\"]);\n",
    "logger = JSONLogger(path=\"./logs.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=5,\n",
    "    n_iter=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba540852-ea6a-4f7e-a8cc-01d9b427ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'target': -1.5207621394968815,\n",
    "#  'params': {'kl_anneal': 1.6660758296625586,\n",
    "#   'num_layers': 2.278738014928133,\n",
    "#   'prior_scale': 6.751488193351199,\n",
    "#   'rho_op': 2.613560914002351,\n",
    "#   'rho_q': 9.386220894044913,\n",
    "#   'sizeof_bnn': 3.3659473914427593,\n",
    "#   'sizeof_layers': 30.1614365856536}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3870f02-cba7-4978-9d6c-4640af602d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
